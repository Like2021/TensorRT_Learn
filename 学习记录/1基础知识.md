# 基础知识记录:pen:

## 基本概念:pencil:



## GPU编程1--GPU中的基本概念: [知乎](https://zhuanlan.zhihu.com/p/150810484)

### 软件资源

1. **线程(Thread)**

2. **线程束(Wrap)**

3. **线程块(Block)**

4. **线程网格(Grid)**





## GPU编程2--CUDA核函数和线程配置: [知乎](https://zhuanlan.zhihu.com/p/151676261)

### 核函数



### 线程配置

***需要注意的是，二维排序中，Thread(0,1)表示第1行第0列的Thread，这跟我们传统中理解的横坐标和纵坐标不太一样；***

***我们定义grid(4,2)表示第一维度有4个索引值，第二个维度有2个索引值，即2行4列***



在CUDA上可以使用内置变量来获取Thread ID和Block ID：

- `threadIdx.[x, y, z]`表示Block内Thread的编号
- `blockIdx.[x, y, z]`表示Gird内Block的编号
- `blockDim.[x, y, z]`表示Block的维度，也就是Block中每个方向上的Thread的数目
- `gridDim.[x, y, z]`表示Gird的维度，也就是Grid中每个方向上Block的数目



这里假设`grid(gridDim.x, girdDim.y, gridDim.z)`和`block(blockDim.x, blockDim.y, blockDim.z)`

当`grid`和`block`的`dim3`均为3D时，可以用如下公式计算线程索引：

```c++
__device__
int getGlobalIdx_3D_3D()
{
    int blockId = blockIdx.z * gridDim.y * gridDim.x + blockIdx.y * gridDim.x + blockIdx.x;
    int threadId = blockId * (blockDim.x * blockDim.y * blockDim.z)
    + threadIdx.z * blockDim.y * blockDim.x + threadIdx.y * blockDim.x + threadIdx.x;
}
```

其中，

`grid`的`Idx`索引是从0开始的，即当`Dim=1`时，`Idx`索引为0，`block`同理。



# 矩阵乘法实例

![image-20230705215330234](1基础知识/image-20230705215330234-1688565212776-1.png)

**平铺矩阵算法的线程配置思路：**

每个`block`对应一个`BLOCK_SIZE`，每个线程对应`BLOCK_SIZE`中的每一行或列。

```c++
// 2.定义BLOCK_SIZE的边界等参数
int bx = blockIdx.x;
int by = blockIdx.y;
int tx = threadIdx.x;
int ty = threadIdx.y;

int row = by * BLOCK_SIZE + ty;
int col = bx * BLOCK_SIZE + tx;
```



## 代码实现

### 核函数

```c++
__global__ void matrixMultiplyShared(float *A, float *B, float *C,
	int numARows, int numAColumns, int numBRows, int numBColumns, int numCRows, int numCColumns)
{
    // 1.定义共享内存
	__shared__ float sharedM[BLOCK_SIZE][BLOCK_SIZE];
	__shared__ float sharedN[BLOCK_SIZE][BLOCK_SIZE];

    // 2.定义BLOCK_SIZE的边界等参数
	int bx = blockIdx.x;
	int by = blockIdx.y;
	int tx = threadIdx.x;
	int ty = threadIdx.y;

	int row = by * BLOCK_SIZE + ty;
	int col = bx * BLOCK_SIZE + tx;

	float Csub = 0.0;

	for (int i = 0; i < (int)(ceil((float)numAColumns / BLOCK_SIZE)); i++)
	{
        // 3.将当前循环内的BLOCK_SIZE子集中的元素存入共享内存
		if (i*BLOCK_SIZE + tx < numAColumns && row < numARows)
			sharedM[ty][tx] = A[row*numAColumns + i * BLOCK_SIZE + tx];
		else
			sharedM[ty][tx] = 0.0;

		if (i*BLOCK_SIZE + ty < numBRows && col < numBColumns)
			sharedN[ty][tx] = B[(i*BLOCK_SIZE + ty)*numBColumns + col];
		else
			sharedN[ty][tx] = 0.0;
		__syncthreads();

		// 4.计算当前循环内的子集结果
		for (int j = 0; j < BLOCK_SIZE; j++)
			Csub += sharedM[ty][j] * sharedN[j][tx];
		__syncthreads();
	}

	// 5.最后写入全局内存中
	if (row < numCRows && col < numCColumns)
		C[row*numCColumns + col] = Csub;

}
```



### 主体函数

```c++
// 初始化host端输入数据的函数
void initial(float *array, int size)
{
	for (int i = 0; i < size; i++)
	{
		array[i] = (float)(rand() % 10 + 1);
	}
}

int main()
{
    // 组织host数据
    int Axy = M * K;
	int Bxy = K * N;
	int Cxy = M * N;
    // 定义host端的指针，开辟相应内存
    float *h_A, *h_B, *hostRef, *deviceRef;
	h_A = (float*)malloc(Axy * sizeof(float));
	h_B = (float*)malloc(Bxy * sizeof(float));

    // 定义host端记录结果的变量
	deviceRef = (float*)malloc(Cxy * sizeof(float));
    
    // 初始化host端输入数据
    initial(h_A, Axy);
    initial(h_B, Bxy);
    
    // 组织gpu资源
    float *d_A, *d_B, *d_C;
	cudaMalloc((void**)&d_A, Axy * sizeof(float));
	cudaMalloc((void**)&d_B, Bxy * sizeof(float));
	cudaMalloc((void**)&d_C, Cxy * sizeof(float));
    cudaMemcpy(d_A, h_A, Axy * sizeof(float), cudaMemcpyHostToDevice);
	cudaMemcpy(d_B, h_B, Bxy * sizeof(float), cudaMemcpyHostToDevice);
    
    // 组织线程配置
    int dimx = 2;
    int dimy = 2;
    dim3 block(dimx, dimy);
    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);
    
    // 调用核函数计算，将结果拷贝回host端
    matrixMultiplyShared << < grid, block >> > (d_A, d_B, d_C, M, K, K, N, M, N);;
    cudaDeviceSynchronize();
    cudaMemcpy(deviceRef, d_C, Cxy * sizeof(float), cudaMemcpyDeviceToHost);
}

```

