# 进阶知识点

## `CUDA Stream`

*CUDA stream是GPU上task的执行队列，所有CUDA操作（kernel，内存拷贝等）都是在stream上执行的。*

有两种类别：

- 隐式流（默认）
- 显式流

其中隐式流里的GPU任务和CPU端计算是同步的，显式流相反。



优点：

• CPU计算和kernel计算并行

• CPU计算和数据传输并行

• 数据传输和kernel计算并行

• kernel计算并行



使用情况：

当数据量和计算量较大的时候使用才会有较好的加速效果，因为流的申请相比`CUDA core`的计算是更更耗时的。



注意：

因为CPU和GPU的数据传输是经过`PCIe`总线的，`PCIe`上的操作是顺序的，所以相同的数据拷贝操作是不能同时进行的。

![image-20230713202221972](2进阶知识/image-20230713202221972-1689250943070-4.png)



### **`API`接口**

```c++
// 定义
cudaStream_t stream;
// 创建
cudaStreamCreate(&stream);
// 数据传输
cudaMemcpyAsync(dst, src, size, type, stream);
// kernel在流中执行
kernel_name<<<grid, block, sharedMemSize, stream>>>(argument list);
// 同步
cudaError_t cudaStreamSynchronize(cudaStream_t stream)
// 查询
cudaError_t cudaStreamQuery(cudaStream_t stream);
// 销毁
cudaError_t cudaStreamDestroy(cudaStream_t stream);
```



### `CUDA`同步操作:timer_clock:

显式同步可以分为四类：

- `cudaDeviceSynchronize`，同步该设备上的所有流
- `cudaStreamSynchronize`，同步一个流





### 实例操作

参考链接：[知乎](https://zhuanlan.zhihu.com/p/51402722)

```c++
#include <cuda_runtime.h>
#include <iostream>

#define STREAMNUMS 10

__global__ void vecAddKernel(float* A_d, float* B_d, float* C_d, int n)
{
    int i = threadIdx.x + blockDim.x * blockIdx.x;
    if (i < n) C_d[i] = A_d[i] + B_d[i];
}

int main(int argc, char* argv[])
{
    // 初始化输入
    int n = atoi(argv[1]);
    std::cout << "计算向量的级数: " << n << std::endl;

    // 申请主机内存
    size_t size = n * sizeof(float);
    float* a = (float*)malloc(size);
    float* b = (float*)malloc(size);
    float* c = (float*)malloc(size);

    for (int i = 0; i < n; i++)
    {
        float af = rand() / double(RAND_MAX);
        float bf = rand() / double(RAND_MAX);
        a[i] = af;
        b[i] = bf;
    }

    // 申请设备指针
    float* a_d = nullptr;
    float* b_d = nullptr;
    float* c_d = nullptr;

    // 线程块所含线程的数量和网格所含线程筷的数量
    int threadPerBlock = 256;
    int blockPerGrid = (n + threadPerBlock - 1) / threadPerBlock;

    // 创建流
    cudaStream_t streams[STREAMNUMS];
    for (int i = 0; i < STREAMNUMS; i++)
    {
        cudaStreamCreate(&streams[i]);
    }

    // 定义偏移量
    int offset = 0;
    // 每个流的数据量大小
    const int dataBlock = n / STREAMNUMS;
    // 循环启动stream
    for (int i = 0; i < STREAMNUMS; i++)
    {
        std::cout << "接下来开始进行流操作" << i << std::endl;
        offset = dataBlock * i;
        cudaMemcpyAsync(a_d + offset, a + offset, dataBlock, cudaMemcpyHostToDevice, streams[i]);
        cudaMemcpyAsync(b_d + offset, b + offset, dataBlock, cudaMemcpyHostToDevice, streams[i]);
        vecAddKernel<<<threadPerBlock, blockPerGrid>>>(a_d, b_d, c_d, dataBlock);
        cudaMemcpyAsync(c + offset, c_d + offset, dataBlock, cudaMemcpyHostToDevice, streams[i]);
    }

    // cudaDeviceSynchronize();
    // 同步流
    for (int i = 0; i < STREAMNUMS; i++)
    {
        cudaStreamSynchronize(streams[i]);
    }
    for (int i = 0; i < STREAMNUMS; i++)
    {
        cudaStreamDestroy(streams[i]);
    }

    cudaFree(a_d);
	cudaFree(b_d);
	cudaFree(c_d);

    std::cout << "正常运行" << std::endl;

	free(a);
	free(b);
	free(c);

    return 0;
}
```







# 矩阵乘法进阶:chart_with_upwards_trend:

代码的对比链接：[CSDN](https://blog.csdn.net/sinat_38368658/article/details/105117534)

使用共享内存，并利用平铺矩阵方法计算

![image-20230705215330234](2进阶知识/image-20230705215330234-1689239776758-2.png)



具体代码：

```c++
#include <iostream>
#include <cuda_runtime.h>

#define M 512
#define K 512
#define N 512

#define BLOCK_SIZE 32 
#define width 512

__global__ void multiplicateMatrixShareMemory(float* Md, float* Nd, float* Pd)
{
    // 开辟共享内存
    __shared__ float Mds[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float Nds[BLOCK_SIZE][BLOCK_SIZE];

    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    // 标识计算结果Pd对应的行列，这样就确认好了矩阵单个元素对应的线程
    int Row = by * BLOCK_SIZE + ty;
    int Col = bx * BLOCK_SIZE + tx;

    float Pvalue = 0;

    // 注: 同一BLOCK_SIZE内的块相加后的结果才是对应的Pd单元素的结果
    // 这里利用for循环，计算同一BLOCK_SIZE内的块
    // width / BLOCK_SIZE对应块的数目
    for (int m = 0; m < width / BLOCK_SIZE; m++)
    {
        // 这里对应
        // Mds[ty][tx] = Md[Row][tx]
        // Nds[ty][tx] = Nd[ty][Col]
        // 但全局内存中二维数组是以一维数组的形式保存的，共享内存则不一样，所以这里需要对Md和Nd进行索引转换
        // 实现从两个矩阵中各取一个元素存入共享内存
        Mds[ty][tx] = Md[Row * width + (m * BLOCK_SIZE + tx)];
        Nds[ty][tx] = Nd[(m * BLOCK_SIZE + ty) * width + Col];
        // 等待所有线程都将对应元素存入共享内存中
        __syncthreads();

        // 累加块相乘后的子集
        for (int k = 0; k < BLOCK_SIZE; k++)
        {
            Pvalue += Mds[ty][k] + Nds[k][tx];
        }
        // 同步结果，保证完全计算完当前块后，再进下一个for循环，计算下一个块
    }
    // 最后把结果写入全局内存Pd中
    Pd[Row * width + Col] = Pvalue;
}

int main(int argc, char* argv[])
{
    // 1.组织数据
	int Axy = M * K;
	int Bxy = K * N;
	int Cxy = M * N;

    float *h_A, *h_B, *deviceRef;
	h_A = (float*)malloc(Axy * sizeof(float));
	h_B = (float*)malloc(Bxy * sizeof(float));
    deviceRef = (float*)malloc(Cxy * sizeof(float));

    initial(h_A, Axy);
    initial(h_B, Bxy);
    printf("1");

    // 2.申请设备端内存
    float *d_A, *d_B, *d_C;
	cudaMalloc((void**)&d_A, Axy * sizeof(float));
	cudaMalloc((void**)&d_B, Bxy * sizeof(float));
	cudaMalloc((void**)&d_C, Cxy * sizeof(float));

    cudaMemcpy(d_A, h_A, Axy * sizeof(float), cudaMemcpyHostToDevice);
	cudaMemcpy(d_B, h_B, Bxy * sizeof(float), cudaMemcpyHostToDevice);

    // 3.组织线程配置，调用核函数
    dim3 grid(width / BLOCK_SIZE, width / BLOCK_SIZE);
    dim3 block(BLOCK_SIZE, BLOCK_SIZE);
    multiplicateMatrixShareMemory<<<grid, block>>> (d_A, d_B, d_C);
    cudaMemcpy(deviceRef, d_C, Cxy * sizeof(float), cudaMemcpyDeviceToHost);

    cudaFree(d_A);
	cudaFree(d_B);
	cudaFree(d_C);

	free(h_A);
	free(h_B);
	free(deviceRef);

    return 0;
}
```



## 线程配置进阶

参考资料：

[知乎](https://zhuanlan.zhihu.com/p/442304996)

[简书](https://www.jianshu.com/p/983e9a516522)

